Like the previous article, the learning rate here is set by combination of [Adam](https://en.wikipedia.org/wiki/Adam) and `L2Regularization`. To avoid a relatively slow decrease of `loss` resulted from a relatively high learning rate in later training period, we need to set a flag `isAEpochDone`, so that at the end of each epoch, the learning rate will be regulated to that of 0.75 time of the original value. 
